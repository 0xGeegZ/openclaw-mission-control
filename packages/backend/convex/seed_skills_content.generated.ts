// Generated by scripts/seed-skills-generate.ts. Do not edit by hand.

export const contentBySlug: Record<string, string> = {
  "address-github-pr-comments": `---
name: address-github-pr-comments
description: Address GitHub PR Comments
disable-model-invocation: true
---

# Address GitHub PR Comments

## Overview

Process outstanding reviewer feedback, apply required fixes, and draft clear
responses for each GitHub pull-request comment.

## Steps

1. **Sync and audit comments**
   - Pull the latest branch changes
   - Open the PR conversation view and read every unresolved comment
   - Group comments by affected files or themes
2. **Plan resolutions**
   - List the requested code edits for each thread
   - Identify clarifications or additional context you must provide
   - Note any dependencies or blockers before implementing changes
3. **Implement fixes**
   - Apply targeted updates addressing one comment thread at a time
   - Run relevant tests or linters after impactful changes
   - Stage changes with commits that reference the addressed feedback
4. **Draft responses**
   - Summarize the action taken or reasoning provided for each comment
   - Link to commits or lines when clarification helps reviewers verify
   - Highlight any remaining questions or follow-up needs

## Response Checklist

- [ ] All reviewer comments acknowledged
- [ ] Required code changes implemented and tested
- [ ] Clarifying explanations prepared for nuanced threads
- [ ] Follow-up items documented or escalated
- [ ] PR status updated for reviewers
`,
  "backend-convex": `---
name: backend-convex
description: Convex schema, queries, mutations, auth guards, and indexes. Follow project patterns in packages/backend/convex.
---

# Backend Convex

Use this skill when working with the Convex backend: schema, queries, mutations, auth guards, and indexes. Follow project patterns in \`packages/backend/convex\`.

## Multi-tenancy (critical)

- Every table except \`accounts\` has \`accountId\`.
- Every query and mutation must scope by \`accountId\`. No cross-account data access.
- Use \`requireAccountMember(ctx, accountId)\` (or \`requireAccountAdmin\` when needed) at the start of user-facing handlers to enforce membership and get \`accountId\` in scope.

## Auth

- **User-facing:** \`requireAuth(ctx)\` for identity only; \`requireAccountMember(ctx, accountId)\` for identity + account membership. Both throw on failure.
- **Service / runtime:** Use \`convex/service/*\` and service auth tokens; do not use user identity in those paths.
- Identity comes from Clerk via \`ctx.auth.getUserIdentity()\`; use \`lib/auth.ts\` helpers only.

## Schema and validators

- Define tables in \`schema.ts\` with \`defineSchema\` and \`defineTable\`. Use \`v\` from \`convex/values\` for all fields.
- Shared union validators (e.g. task status, agent status) live in \`lib/validators.ts\` and are reused in schema and args.
- Define an index for every query pattern; avoid full table scans. Use \`.index("by_account", ["accountId"])\` and compound indexes like \`["accountId", "status"]\` as needed.

## Queries and mutations

- Always use \`.withIndex()\` for queries; match the index fields to the filter (e.g. \`by_account\`, \`by_account_status\`, \`by_account_user\`).
- Use \`.unique()\` for at-most-one results, \`.first()\` or \`.collect()\` for lists. Prefer indexing over \`.filter()\` on large sets.
- Use \`internalQuery\` / \`internalMutation\` for server-only callers (e.g. standup, cron); use \`query\` / \`mutation\` for the HTTP/API surface and always guard with auth.
- Await all async work (e.g. \`ctx.scheduler.runAfter\`, \`ctx.db.patch\`) to avoid floating promises.

## Activity and notifications

- Log meaningful state changes with \`logActivity\` from \`lib/activity.ts\` (e.g. task status, assignments, doc updates).
- Use helpers in \`lib/notifications.ts\` for creating notifications (mentions, assignments, status changes). Notifications are consumed by the runtime for delivery.

## Conventions

- **File naming:** snake_case for all files under \`convex/\` (e.g. \`tasks.ts\`, \`seed_skills_build.ts\`).
- **Imports:** Use \`api\` from \`_generated/api\`, \`Id\`/\`Doc\` from \`_generated/dataModel\`, and server types from \`_generated/server\`.
- **Args:** Validate all arguments with \`v.*\` validators; reuse shared validators from \`lib/validators.ts\` where applicable.

## References

- Convex docs: [Schemas](https://docs.convex.dev/database/schemas), [Indexes](https://docs.convex.dev/database/reading-data/indexes), [Best practices](https://docs.convex.dev/production/best-practices).
- Project: \`packages/backend/convex/README.md\`, \`convex/schema.ts\`, \`convex/lib/auth.ts\`.
`,
  "clarify-task": `---
name: clarify-task
description: Clarify Task
disable-model-invocation: true
---

# Clarify Task

Before doing ANY coding work on the task I describe:

1. **Ask clarifying questions** - Use 2-4 multiple choice questions to clarify requirements:
   - Data flow and architecture
   - APIs and integrations
   - Authentication/authorization
   - Edge cases and error handling
   - UI/UX expectations (if applicable)

2. **Restate requirements** - After I answer, restate the final requirements in your own words to confirm understanding.

3. **Confirm before proceeding** - ONLY then ask if I want to proceed to planning/implementation.

Keep asking questions until you have enough context to give an accurate & confident answer.
`,
  "code-review-checklist": `---
name: code-review-checklist
description: Code Review
disable-model-invocation: true
---

# Code Review

## Overview

Perform a thorough code review that verifies functionality, maintainability, and
security before approving a change. Focus on architecture, readability,
performance implications, and provide actionable suggestions for improvement.

## Steps

1. **Understand the change**
   - Read the PR description and related issues for context
   - Identify the scope of files and features impacted
   - Note any assumptions or questions to clarify with the author
2. **Validate functionality**
   - Confirm the code delivers the intended behavior
   - Exercise edge cases or guard conditions mentally or by running locally
   - Check error handling paths and logging for clarity
3. **Assess quality**
   - Ensure functions are focused, names are descriptive, and code is readable
   - Watch for duplication, dead code, or missing tests
   - Verify documentation and comments reflect the latest changes
4. **Review security and risk**
   - Look for injection points, insecure defaults, or missing validation
   - Confirm secrets or credentials are not exposed
   - Evaluate performance or scalability impacts of the change

## Review Checklist

### Functionality

- [ ] Intended behavior works and matches requirements
- [ ] Edge cases handled gracefully
- [ ] Error handling is appropriate and informative

### Code Quality

- [ ] Code structure is clear and maintainable
- [ ] No unnecessary duplication or dead code
- [ ] Tests/documentation updated as needed

### Security & Safety

- [ ] No obvious security vulnerabilities introduced
- [ ] Inputs validated and outputs sanitized
- [ ] Sensitive data handled correctly

## Additional Review Notes

- Architecture and design decisions considered
- Performance bottlenecks or regressions assessed
- Coding standards and best practices followed
- Resource management, error handling, and logging reviewed
- Suggested alternatives, additional test cases, or documentation updates
  captured

Provide constructive feedback with concrete examples and actionable guidance for
the author.
`,
  "commit": `---
name: commit
description: Commit current work
disable-model-invocation: true
---

# Commit current work

Commit current work.

## Important

- Prepend GIT_EDITOR=true to all git commands you run, especially the ones looking at diffs, so you can avoid getting blocked as you execute commands
- If you can't get any information from git diff, just using your working memory to determine what has changed

## Instructions

Review each file individually to make sure they're related to the work you just did, then write a brief commit message in the following format:

- Short title description (< 80 characters)
- 2~3 bullet points (< 80 characters) with a quick description

## Notes

- You should only commit work when instructed. Do not keep committing subsquent work unless explicitly told so

Optional: ask me if I would like to push the commit. (Only if I'm not on main)
`,
  "create-pr": `---
name: create-pr
description: PR Creation Command
disable-model-invocation: true
---

---

description: |
Complete PR creation workflow: analyze changes, stage/commit if needed,
create branch (if not already), generate PR title & description, and
submit PR using GitHub CLI. Handles the full end-to-end process.
globs:

- "\\*_/_"
  alwaysApply: false

---

# PR Creation Command

When invoked, execute the following workflow:

## 1. Analyze Current State

**Check git status:**

\`\`\`bash
git status
git diff --stat
git log --oneline main..HEAD
\`\`\`

**Determine action needed:**

- If uncommitted changes exist ‚Üí stage and commit them
- If on main/master ‚Üí create feature branch
- If commits exist on branch ‚Üí check if PR already exists

## 2. Handle Uncommitted Changes (if present)

**Stage relevant files:**

\`\`\`bash
git add <files>
\`\`\`

**Generate commit message:**

- Format: \`type(scope): description\`
- Types: \`feat\`, \`fix\`, \`refactor\`, \`test\`, \`docs\`, \`chore\`, \`perf\`
- Keep under 72 characters
- Example: \`feat(rules): preserve context on rule blocking\`

**Commit:**

\`\`\`bash
git commit -m "type(scope): description"
\`\`\`

## 3. Ensure Feature Branch (if needed)

**If currently on main/master, create branch:**

- Format: \`<type>/<short-description>\`
- Example: \`feat/context-on-failure\`, \`fix/rule-evaluation\`
- Keep concise, use kebab-case

\`\`\`bash
git checkout -b <branch-name>
\`\`\`

## 4. Analyze Changes for PR

**Review diff scope:**

\`\`\`bash
git diff main --stat
git diff main --name-status
\`\`\`

**Group changes by purpose:**

- Core changes (main feature/fix)
- Tests (new/updated test coverage)
- Documentation (README, comments)
- Dependencies (package.json updates)
- Configuration (build, lint, env)

## 5. Generate PR Content

**Title (< 72 chars):**

- Format: \`Type: Brief description of main change\`
- Example: \`Feat: Preserve context when rules block evaluation\`
- Capitalize first word, no period at end

**Description structure:**

\`\`\`markdown
## Summary

[One paragraph explaining what this PR does and why]

## Changes

**üîß Core Changes**
_[Brief description of main implementation]_

- \`file1.ts\`: description
- \`file2.ts\`: description

**‚úÖ Tests**
_[Brief description of test coverage]_

- \`test1.spec.ts\`: description
- \`test2.spec.ts\`: description

**üìö Documentation** (if applicable)
_[Brief description of doc updates]_

- \`README.md\`: description

## Related

Closes #[issue-number] (if applicable)
Ref #[issue-number] (if related but not closing)

## Testing

1. Run \`npm test\` and verify all tests pass
2. [Specific test scenario 1]
3. [Specific test scenario 2]
\`\`\`

**Guidelines:**

- Keep descriptions concise and actionable
- No code snippets in PR description
- Defer detailed rationale to linked issues
- Focus on WHAT changed, not WHY (unless critical)

## 6. Create PR with GitHub CLI

**Check if PR exists:**

\`\`\`bash
gh pr list --head \$(git branch --show-current)
\`\`\`

**Create PR if it doesn't exist:**

\`\`\`bash
gh pr create \\
  --title "<PR Title>" \\
  --body "<PR Description>" \\
  --base main
\`\`\`

**Options to consider:**

- \`--draft\` - Create as draft PR
- \`--assignee @me\` - Auto-assign to self
- \`--reviewer <username>\` - Request review

## 7. Confirm and Output

**Display:**

- ‚úÖ Branch: \`<branch-name>\`
- ‚úÖ Commits: \`<commit-count>\` commit(s)
- ‚úÖ PR created: \`<PR-URL>\`
- üìù Title: \`<PR-title>\`

**Next steps for user:**

- Review PR at the provided URL
- Wait for CI checks to complete
- Address review feedback if needed

---

## Example Usage

**User says:** "Create a PR for my changes"

**Command executes:**

1. Checks git status ‚Üí finds uncommitted changes
2. Stages files ‚Üí commits with \`feat(rules): preserve context on blocking\`
3. Already on feature branch ‚Üí continues
4. Analyzes diff ‚Üí groups changes
5. Generates PR title and description
6. Creates PR via \`gh pr create\`
7. Outputs PR URL and summary
`,
  "debug-issue": `---
name: debug-issue
description: Debug Issue
disable-model-invocation: true
---

# Debug Issue

## Overview

Help debug the current issue in the code by walking through the debugging process systematically and providing clear, actionable solutions.

## Steps

1. **Problem Analysis**
   - Identify the specific problem or error
   - Understand the expected vs actual behavior
   - Trace the execution flow to find the root cause
2. **Debugging Strategy**
   - Add appropriate logging statements
   - Suggest debugging tools and techniques
   - Identify key variables and states to monitor
   - Recommend breakpoint locations
3. **Solution Approach**
   - Propose potential fixes with explanations
   - Consider multiple solution approaches
   - Evaluate trade-offs of different approaches
   - Provide step-by-step resolution plan
4. **Prevention**
   - Suggest ways to prevent similar issues
   - Recommend additional tests or checks
   - Identify code patterns that could be improved

## Debug Issue Checklist

- [ ] Identified the specific problem or error
- [ ] Understood expected vs actual behavior
- [ ] Traced execution flow to find root cause
- [ ] Added appropriate logging statements
- [ ] Proposed potential fixes with explanations
- [ ] Evaluated trade-offs of different approaches
- [ ] Provided step-by-step resolution plan
- [ ] Suggested ways to prevent similar issues
- [ ] Recommended additional tests or checks
`,
  "deslop": `---
name: deslop
description: Remove AI code slop
disable-model-invocation: true
---

# Remove AI code slop

Check the diff against main, and remove all AI generated slop introduced in this branch.

This includes:

- Commented-out code that should be removed or implemented (not TODO/FIXME comments - keep those)
- Casts to \`any\` to get around type issues (fix with proper types instead)
- Extra defensive checks or try/catch blocks that are abnormal for that area of the codebase (especially if called by
  trusted / validated codepaths)
- Comments that ONLY restate what the code does without adding context (keep comments that explain "why" or provide
  important context)
- Avoid re exporting type from a different file

**IMPORTANT**: Be conservative with comments. Only remove comments that are clearly redundant or explain obvious things.

Keep:

- TODO/FIXME comments (they mark work to be done)
- JSDoc comments that document functions/components
- Comments explaining "why" something is done
- Comments providing context about non-obvious logic
- Comments that match the style of the existing codebase

Report at the end with only a 1-3 sentence summary of what you changed
`,
  "fix-merge-conflict": `---
name: fix-merge-conflict
description: Fix all merge conflicts on the current Git branch non-interactively
disable-model-invocation: true
---

Fix all merge conflicts on the current Git branch non-interactively and make the repo buildable and tested.

Requirements and constraints:

- Operate from the repository root. If not in a Git repo, stop and report.
- Do not ask the user for input. Choose sensible defaults and explain decisions in a brief summary.
- Prefer minimal, correct changes that preserve both sides' intent when possible.
- Use non-interactive flags for any tools you invoke.
- Do not push or tag; only commit locally.

High-level plan:

1. Detect conflicts
   - Run: git status --porcelain | cat
   - Collect files with conflict markers (U statuses or files containing <<<<<<< / ======= / >>>>>>>).
2. Resolve conflicts per file
   - Open each conflicting file and remove conflict markers.
   - Merge both sides logically when feasible. If mutually exclusive, pick the variant that:
     - Compiles and passes type checks, and
     - Preserves existing public APIs and behavior.
   - Language-aware strategy:
     - package.json/pnpm-lock.yaml/yarn.lock: merge keys conservatively; run install to regenerate lockfiles.
     - .lock files (package-lock.json, yarn.lock, pnpm-lock.yaml): prefer regenerating via the package manager rather than manual edits.
     - Generated files and build artifacts: prefer keeping them out of version control if applicable; otherwise prefer current branch (ours).
     - Config files: preserve union of safe settings; avoid deleting required fields.
     - Text/markdown: include both unique content, deduplicate headings.
     - Binary files: prefer current branch (ours) unless project docs indicate otherwise.
3. Validate
   - If Node/TypeScript/JS present: install deps if manifests changed (use --frozen-lockfile false equivalents), then run lint/typecheck/build/tests if available.
   - If other ecosystems detected (Python, Go, etc.), run their standard build/tests when available.
4. Finalize
   - Stage all resolved files and any regenerated lockfiles.
   - Create a single commit with message: "chore: resolve merge conflicts".
   - Output a concise summary of files touched and notable resolution choices.

Operational guidance:

- Assume the user isn't available; make best-effort decisions. If a resolution is ambiguous and blocks build/tests, prefer the variant that compiles and green-tests.
- If a file still contains conflict markers after your first pass, revisit and resolve them before proceeding.
- For large refactors causing conflicts, prefer keeping consistent imports, types, and module boundaries. Use exhaustive switch guards in TypeScript and explicit type annotations where needed.
- Keep edits minimal and readable; avoid reformatting unrelated code.

Deliverables:

- A clean working tree with all conflicts resolved.
- Successful build/tests where applicable.
- One local commit containing the resolutions.
`,
  "frontend-nextjs": `---
name: vercel-react-best-practices
description: React and Next.js performance optimization guidelines from Vercel Engineering. This skill should be used when writing, reviewing, or refactoring React/Next.js code to ensure optimal performance patterns. Triggers on tasks involving React components, Next.js pages, data fetching, bundle optimization, or performance improvements.
license: MIT
metadata:
  author: vercel
  version: "1.0.0"
---

# Vercel React Best Practices

Comprehensive performance optimization guide for React and Next.js applications, maintained by Vercel. Contains 57 rules across 8 categories, prioritized by impact to guide automated refactoring and code generation.

## When to Apply

Reference these guidelines when:
- Writing new React components or Next.js pages
- Implementing data fetching (client or server-side)
- Reviewing code for performance issues
- Refactoring existing React/Next.js code
- Optimizing bundle size or load times

## Rule Categories by Priority

| Priority | Category | Impact | Prefix |
|----------|----------|--------|--------|
| 1 | Eliminating Waterfalls | CRITICAL | \`async-\` |
| 2 | Bundle Size Optimization | CRITICAL | \`bundle-\` |
| 3 | Server-Side Performance | HIGH | \`server-\` |
| 4 | Client-Side Data Fetching | MEDIUM-HIGH | \`client-\` |
| 5 | Re-render Optimization | MEDIUM | \`rerender-\` |
| 6 | Rendering Performance | MEDIUM | \`rendering-\` |
| 7 | JavaScript Performance | LOW-MEDIUM | \`js-\` |
| 8 | Advanced Patterns | LOW | \`advanced-\` |

## Quick Reference

### 1. Eliminating Waterfalls (CRITICAL)

- \`async-defer-await\` - Move await into branches where actually used
- \`async-parallel\` - Use Promise.all() for independent operations
- \`async-dependencies\` - Use better-all for partial dependencies
- \`async-api-routes\` - Start promises early, await late in API routes
- \`async-suspense-boundaries\` - Use Suspense to stream content

### 2. Bundle Size Optimization (CRITICAL)

- \`bundle-barrel-imports\` - Import directly, avoid barrel files
- \`bundle-dynamic-imports\` - Use next/dynamic for heavy components
- \`bundle-defer-third-party\` - Load analytics/logging after hydration
- \`bundle-conditional\` - Load modules only when feature is activated
- \`bundle-preload\` - Preload on hover/focus for perceived speed

### 3. Server-Side Performance (HIGH)

- \`server-auth-actions\` - Authenticate server actions like API routes
- \`server-cache-react\` - Use React.cache() for per-request deduplication
- \`server-cache-lru\` - Use LRU cache for cross-request caching
- \`server-dedup-props\` - Avoid duplicate serialization in RSC props
- \`server-serialization\` - Minimize data passed to client components
- \`server-parallel-fetching\` - Restructure components to parallelize fetches
- \`server-after-nonblocking\` - Use after() for non-blocking operations

### 4. Client-Side Data Fetching (MEDIUM-HIGH)

- \`client-swr-dedup\` - Use SWR for automatic request deduplication
- \`client-event-listeners\` - Deduplicate global event listeners
- \`client-passive-event-listeners\` - Use passive listeners for scroll
- \`client-localstorage-schema\` - Version and minimize localStorage data

### 5. Re-render Optimization (MEDIUM)

- \`rerender-defer-reads\` - Don't subscribe to state only used in callbacks
- \`rerender-memo\` - Extract expensive work into memoized components
- \`rerender-memo-with-default-value\` - Hoist default non-primitive props
- \`rerender-dependencies\` - Use primitive dependencies in effects
- \`rerender-derived-state\` - Subscribe to derived booleans, not raw values
- \`rerender-derived-state-no-effect\` - Derive state during render, not effects
- \`rerender-functional-setstate\` - Use functional setState for stable callbacks
- \`rerender-lazy-state-init\` - Pass function to useState for expensive values
- \`rerender-simple-expression-in-memo\` - Avoid memo for simple primitives
- \`rerender-move-effect-to-event\` - Put interaction logic in event handlers
- \`rerender-transitions\` - Use startTransition for non-urgent updates
- \`rerender-use-ref-transient-values\` - Use refs for transient frequent values

### 6. Rendering Performance (MEDIUM)

- \`rendering-animate-svg-wrapper\` - Animate div wrapper, not SVG element
- \`rendering-content-visibility\` - Use content-visibility for long lists
- \`rendering-hoist-jsx\` - Extract static JSX outside components
- \`rendering-svg-precision\` - Reduce SVG coordinate precision
- \`rendering-hydration-no-flicker\` - Use inline script for client-only data
- \`rendering-hydration-suppress-warning\` - Suppress expected mismatches
- \`rendering-activity\` - Use Activity component for show/hide
- \`rendering-conditional-render\` - Use ternary, not && for conditionals
- \`rendering-usetransition-loading\` - Prefer useTransition for loading state

### 7. JavaScript Performance (LOW-MEDIUM)

- \`js-batch-dom-css\` - Group CSS changes via classes or cssText
- \`js-index-maps\` - Build Map for repeated lookups
- \`js-cache-property-access\` - Cache object properties in loops
- \`js-cache-function-results\` - Cache function results in module-level Map
- \`js-cache-storage\` - Cache localStorage/sessionStorage reads
- \`js-combine-iterations\` - Combine multiple filter/map into one loop
- \`js-length-check-first\` - Check array length before expensive comparison
- \`js-early-exit\` - Return early from functions
- \`js-hoist-regexp\` - Hoist RegExp creation outside loops
- \`js-min-max-loop\` - Use loop for min/max instead of sort
- \`js-set-map-lookups\` - Use Set/Map for O(1) lookups
- \`js-tosorted-immutable\` - Use toSorted() for immutability

### 8. Advanced Patterns (LOW)

- \`advanced-event-handler-refs\` - Store event handlers in refs
- \`advanced-init-once\` - Initialize app once per app load
- \`advanced-use-latest\` - useLatest for stable callback refs

## How to Use

Read individual rule files for detailed explanations and code examples:

\`\`\`
rules/async-parallel.md
rules/bundle-barrel-imports.md
\`\`\`

Each rule file contains:
- Brief explanation of why it matters
- Incorrect code example with explanation
- Correct code example with explanation
- Additional context and references

## Full Compiled Document

For the complete guide with all rules expanded: \`AGENTS.md\`
`,
  "generate-pr-description": `---
name: generate-pr-description
description: Generate PR Description
disable-model-invocation: true
---

# Generate PR Description

## Overview

Create a comprehensive pull request description based on the changes in this branch and format it as proper markdown for use in a GitHub PR description.

## Steps

1. **Summary**
   - Provide a clear, concise summary of what this PR accomplishes
2. **Changes Made**
   - List the key changes made in this PR
   - Include both code and non-code changes
   - Highlight any breaking changes
3. **Testing**
   - Describe how the changes were tested
   - Include any new test cases added
   - Note any manual testing performed
4. **Related Issues**
   - Link to any related issues or tickets
   - Use closing keywords if this PR resolves issues
5. **Additional Notes**
   - Any deployment considerations
   - Follow-up work required
   - Notes for reviewers

## Generate PR Description Checklist

- [ ] Provided clear, concise summary of what this PR accomplishes
- [ ] Listed all key changes made in this PR
- [ ] Highlighted any breaking changes
- [ ] Described how the changes were tested
- [ ] Included any new test cases added
- [ ] Noted any manual testing performed
- [ ] Linked to any related issues or tickets
- [ ] Included any deployment considerations
- [ ] Noted any follow-up work required
- [ ] Formatted as proper markdown for GitHub PR
`,
  "github-issue-triage": `---
name: github-issue-triage
description: Triage GitHub issues: label, prioritize, assign. Check repo CONTRIBUTING and issue templates.
---

# GitHub issue triage

Use this skill when triaging GitHub issues: label, prioritize, assign. Check repo CONTRIBUTING and issue templates.
`,
  "plan-feature": `---
name: plan-feature
description: Setup & Plan a Feature (Cursor Plan Mode)
disable-model-invocation: true
---

# Setup & Plan a Feature (Cursor Plan Mode)

## Overview

Systematically set up a new feature from initial planning through to a detailed implementation plan. You are in **planning mode**: do not write or change any code, only plan. Do not paste full implementations; only short code snippets or signatures if absolutely needed to clarify the plan.

The plan must be detailed enough that:

- If we give it to **100 different engineers**, at least **85%** of the resulting code would be very similar.
- A **junior engineer** can implement the feature by following the plan step by step.
- Running command .cursor/commands/code-review-checklist.md agains the plan you gonna write should not return any issues or warnings.

---

## Phase 0: Setup (before writing the plan)

1. **Define requirements**
   - Clarify feature scope and goals with the user (or from the description below).
   - Identify user stories and acceptance criteria.
   - Note any technical approach constraints.

2. **Scope the repo**
   - Identify which **apps/packages** in this repo are affected (e.g. \`apps/web\`, \`packages/backend\`, \`packages/ui\`).
   - Note any **key assumptions** and mark them clearly (e.g. "Assumption A: ‚Ä¶") so they can be confirmed later.

3. **Feature setup checklist** (confirm before proceeding)
   - [ ] Requirements / scope documented
   - [ ] User stories or acceptance criteria clear
   - [ ] Feature branch created (if applicable)
   - [ ] Development environment ready

---

## Phase 1: Understand the task

- Read the user's description below this command carefully.
- Skim the relevant files in this repo:
  - Routes / pages
  - Components
  - Hooks / state management
  - API handlers / services (e.g. Convex queries, mutations, actions)
  - Schemas / models / types
  - Config files and environment usage
  - Existing tests related to this area
- Use **project-wide search** and **symbol navigation** to understand how similar features are implemented today.
- Before asking the user anything, **thoroughly analyze the codebase and existing documentation** to try to answer your own questions:
  - Reuse existing patterns, utilities, hooks, and types whenever possible.
  - Only ask clarifying questions that you **cannot reasonably resolve yourself** after checking the codebase and docs.
- If anything is still unclear after that, ask up to **5 short clarifying questions** before writing the final plan.

---

## Phase 2: Write a Markdown plan with these sections

### 1. Context & goal

- One short paragraph on what we are building and why.
- Key constraints (tech stack, performance, security, backwards compatibility, UX).

### 2. Codebase research summary

- List the **main files and modules you inspected**, with paths (e.g. \`apps/web/src/app/(dashboard)/[accountSlug]/tasks/page.tsx\`).
- Briefly summarize what you learned:
  - Existing patterns you will follow.
  - Existing APIs, components, hooks, or utilities to reuse.
  - Any important types or schemas that matter for this feature.

### 3. High-level design

- Architecture summary across layers (frontend, backend, shared libraries).
- Main data flows and where they start/end (e.g. "User action ‚Üí React component ‚Üí hook ‚Üí Convex mutation ‚Üí DB ‚Üí response ‚Üí UI update").
- Mention important existing functions, components, hooks, and types by their **exact names** where relevant.
- Explain how the new feature fits into the existing architecture.

### 4. File & module changes

- List **existing files to touch** with their paths.
- List **new files to create** with their paths.
- For each file, write 1‚Äì3 bullets on what will change, with enough detail that another engineer can implement it without guessing, e.g.:
  - New props, fields, or params (with types).
  - New API endpoints or Convex functions (method/name, input/output shape).
  - New state, hooks, or context usage.
  - New utility functions or modules.

### 5. Step-by-step tasks

- Write a **numbered list** of small, atomic steps.
- Each step should be "doable in one focused commit".
- Steps must mention concrete files, functions, and components to edit or create.
- Include any required:
  - Migrations
  - Feature flags
  - Configuration or environment variable changes
- Make the steps explicit enough that a **junior engineer** can follow them one by one without making design decisions on their own.

### 6. Edge cases & risks

- Edge cases to handle (validation, empty states, error handling, race conditions, permissions, auth, rate limits, etc.).
- Potential breaking changes or risky areas in the codebase:
  - Explain why they are risky.
  - Suggest mitigation (feature flags, extra tests, monitoring, fallbacks).

### 7. Testing strategy

- What to cover with **unit tests**: modules, pure logic, input validation, branching logic.
- What to cover with **integration/e2e tests**: critical flows, API + UI integration, cross-service behavior.
- Manual QA checklist: happy path and key edge cases as bullet points (e.g. "Create X‚Ä¶", "Update Y‚Ä¶", "Error when Z‚Ä¶").

### 8. Rollout / migration (if relevant)

- How to deploy safely: feature flags, gradual rollout, kill switch, monitoring dashboards.
- How to migrate existing data or users: one-off scripts, background jobs, or lazy migration.
- Any observability or logging changes: new logs, metrics, or traces to add.

### 9. TODO checklist

- At the end of the plan, create a **detailed TODO list** using Markdown checkboxes (\`- [ ]\`).
- This TODO list should contain **all concrete steps** needed to complete the implementation, from first code change to final deployment and QA.
- Group TODOs when useful, e.g. **Backend**, **Frontend**, **Tests**, **Infra / DevOps**.
- Each TODO item should be clear, small, and directly actionable.

---

## Phase 3: Output rules

- Use clean Markdown headings and bullet points.
- Keep sentences short and concrete.
- Avoid vague statements like "update things as needed"; always be specific about **what**, **where**, and **how**.
- Stay in planning mode only; do not modify code or open pull requests in this step.
`,
  "pr-review-comments": `---
name: pr-review-comments
description: PR Review Comments
disable-model-invocation: true
---

# PR Review Comments

## Goal

Generate **copy/paste-ready pull request review comments** for the current branch, with clear severity and precise code
anchors so the comments can be posted directly on GitHub.

## Inputs

- Prefer the **current git branch** diff against \`origin/main\` unless the user provides a PR base branch.
- If the user provides **manual testing notes / logs**, incorporate them as additional comments and map them to likely
  code locations.

## What to do

1. **Collect context**
   - Determine current branch name and whether the working tree is clean.
   - Identify base branch (default to \`origin/main\`) and list changed files (\`git diff --name-status\`).
   - Skim the highest-impact files (routes, server actions, validation, shared components).
2. **Find reviewable risks**
   - Security/authZ/authN gaps (server-side enforcement vs UI-only gating).
   - Data mapping correctness (form ‚Üí API, API ‚Üí UI, empty/null semantics).
   - Error handling and user-facing messaging (including i18n).
   - Type safety pitfalls (\`any\`, mismatched return shapes).
   - UX issues noted in testing (multi-select, multi-value inputs, loading states, navigation).
   - Performance/regression risks (unnecessary re-renders, sequential server calls).
3. **Produce PR review comments**
   - Output **multiple separate comments** (not one blob), each scoped to one issue.
   - For each comment, include **exact anchor(s)** so the reviewer can place it accurately in the diff.

## Output format (strict)

Return a list of comments using this format exactly:

- **[Blocking|Non-blocking] <Short title>**
  - **Where**: \`<path>\` ‚Üí \`<function/component/section>\`
  - **Comment to paste**:
    > <The exact PR comment text as you'd write it on GitHub.>
  - **Why**: <One sentence, user impact or maintenance risk.>
  - **Suggested fix**: <One sentence, specific action.>

## Constraints

- Write in **English**.
- Prefer the repository's conventions and existing utilities.
- Do **not** propose large refactors unless necessary; prioritize high-signal, actionable comments.
- If something is unclear, ask targeted clarifying questions instead of guessing.
`,
  "pr-review": `---
name: requesting-code-review
description: Use when completing tasks, implementing major features, or before merging to verify work meets requirements
---

# Requesting Code Review

Dispatch superpowers:code-reviewer subagent to catch issues before they cascade.

**Core principle:** Review early, review often.

## When to Request Review

**Mandatory:**
- After each task in subagent-driven development
- After completing major feature
- Before merge to main

**Optional but valuable:**
- When stuck (fresh perspective)
- Before refactoring (baseline check)
- After fixing complex bug

## How to Request

**1. Get git SHAs:**
\`\`\`bash
BASE_SHA=\$(git rev-parse HEAD~1)  # or origin/main
HEAD_SHA=\$(git rev-parse HEAD)
\`\`\`

**2. Dispatch code-reviewer subagent:**

Use Task tool with superpowers:code-reviewer type, fill template at \`code-reviewer.md\`

**Placeholders:**
- \`{WHAT_WAS_IMPLEMENTED}\` - What you just built
- \`{PLAN_OR_REQUIREMENTS}\` - What it should do
- \`{BASE_SHA}\` - Starting commit
- \`{HEAD_SHA}\` - Ending commit
- \`{DESCRIPTION}\` - Brief summary

**3. Act on feedback:**
- Fix Critical issues immediately
- Fix Important issues before proceeding
- Note Minor issues for later
- Push back if reviewer is wrong (with reasoning)

## Example

\`\`\`
[Just completed Task 2: Add verification function]

You: Let me request code review before proceeding.

BASE_SHA=\$(git log --oneline | grep "Task 1" | head -1 | awk '{print \$1}')
HEAD_SHA=\$(git rev-parse HEAD)

[Dispatch superpowers:code-reviewer subagent]
  WHAT_WAS_IMPLEMENTED: Verification and repair functions for conversation index
  PLAN_OR_REQUIREMENTS: Task 2 from docs/plans/deployment-plan.md
  BASE_SHA: a7981ec
  HEAD_SHA: 3df7661
  DESCRIPTION: Added verifyIndex() and repairIndex() with 4 issue types

[Subagent returns]:
  Strengths: Clean architecture, real tests
  Issues:
    Important: Missing progress indicators
    Minor: Magic number (100) for reporting interval
  Assessment: Ready to proceed

You: [Fix progress indicators]
[Continue to Task 3]
\`\`\`

## Integration with Workflows

**Subagent-Driven Development:**
- Review after EACH task
- Catch issues before they compound
- Fix before moving to next task

**Executing Plans:**
- Review after each batch (3 tasks)
- Get feedback, apply, continue

**Ad-Hoc Development:**
- Review before merge
- Review when stuck

## Red Flags

**Never:**
- Skip review because "it's simple"
- Ignore Critical issues
- Proceed with unfixed Important issues
- Argue with valid technical feedback

**If reviewer wrong:**
- Push back with technical reasoning
- Show code/tests that prove it works
- Request clarification

See template at: requesting-code-review/code-reviewer.md
`,
  "production-ready-refactor": `---
name: production-ready-refactor
description: Production-Ready Refactor
disable-model-invocation: true
---

# Production-Ready Refactor

You are refactoring an existing implementation to production-ready quality.

Follow this workflow:

## 1) Understand the current behavior

- Read the relevant code paths end-to-end.
- Identify integration points, config, and tests that touch this area.
- Summarize intended behavior and any gaps/risks.

## 2) Reuse before creating

- Search for existing utilities, hooks, types, and patterns in the codebase.
- Extend existing code where possible; avoid new files unless necessary.
- Keep imports at the top of the file.
- Follow DRY and KISS: remove duplication, keep functions focused and small.

## 3) Refactor to match project standards

- Match existing file organization, naming conventions, and style.
- Use \`@\` imports, established helpers, and shared types.
- Add JSDoc for exported functions/components/hooks.
- Add comments only when they explain non-obvious intent or constraints.

## 4) Production readiness checks

- Handle edge cases and error paths explicitly.
- Validate configuration/environment usage and update \`.env.example\` + \`Env\` when needed.
- Ensure logging and error messages are actionable.
- Avoid breaking public URLs or APIs; add redirects/rewrites if required.

## 5) Tests & verification

- Add or update tests for critical logic and regressions.
- Run \`pnpm tsc --noEmit\` and \`pnpm lint\`.
- Run \`pnpm build\` if the change impacts build-time behavior.

## 6) Final response

- Provide a concise change summary.
- List tests run and their results.
- Call out any remaining risks, manual QA steps, or follow-ups.
`,
  "release-management": `---
name: release-management
description: Release checklists, changelogs, and versioning. Align with the squad lead and any existing release process.
---

# Release management

Use this skill for release checklists, changelogs, and versioning. Align with the squad lead and any existing release process.

## When to use

- Preparing or executing a release (app, package, or runtime).
- Writing or updating changelogs and release notes.
- Deciding version bumps (semver: major/minor/patch) and release cadence.
- Coordinating with the squad lead on timing, scope, and rollback.

## Release checklist (outline)

1. **Pre-release**
   - Confirm all release-blocking work is done (tests, docs, migrations if any).
   - Update version in the right place(s) (e.g. \`package.json\`, app config).
   - Draft or update changelog/release notes for the version.
   - Get squad lead sign-off if required.

2. **Cut release**
   - Tag or cut the release (e.g. Git tag, GitHub release, package publish).
   - Document the exact artifact or commit for the release.

3. **Post-release**
   - Announce or notify as per process.
   - Update any ‚Äúlatest‚Äù or ‚Äúcurrent‚Äù references.
   - Note any follow-ups (patches, docs, rollback steps).

## Changelog and versioning

- Prefer a single source of truth (e.g. \`CHANGELOG.md\` or GitHub Releases) with a consistent format (e.g. ‚ÄúAdded / Changed / Fixed‚Äù or keep-a-changelog).
- Use semantic versioning (semver) unless the project specifies otherwise: major for breaking changes, minor for new features, patch for fixes.
- Tie each changelog entry to a version and date; link to commits or PRs where helpful.

## Collaboration

- Align with the squad lead on what counts as release-blocking and who approves releases.
- Reuse existing templates or runbooks if the project has them; otherwise propose a minimal checklist and iterate.
`,
  "repo-architecture": `---
name: c4-architecture
description: Generate architecture documentation using C4 model Mermaid diagrams. Use when asked to create architecture diagrams, document system architecture, visualize software structure, create C4 diagrams, or generate context/container/component/deployment diagrams. Triggers include "architecture diagram", "C4 diagram", "system context", "container diagram", "component diagram", "deployment diagram", "document architecture", "visualize architecture".
---

# C4 Architecture Documentation

Generate software architecture documentation using C4 model diagrams in Mermaid syntax.

## Workflow

1. **Understand scope** - Determine which C4 level(s) are needed based on audience
2. **Analyze codebase** - Explore the system to identify components, containers, and relationships
3. **Generate diagrams** - Create Mermaid C4 diagrams at appropriate abstraction levels
4. **Document** - Write diagrams to markdown files with explanatory context

## C4 Diagram Levels

Select the appropriate level based on the documentation need:

| Level | Diagram Type | Audience | Shows | When to Create |
|-------|-------------|----------|-------|----------------|
| 1 | **C4Context** | Everyone | System + external actors | Always (required) |
| 2 | **C4Container** | Technical | Apps, databases, services | Always (required) |
| 3 | **C4Component** | Developers | Internal components | Only if adds value |
| 4 | **C4Deployment** | DevOps | Infrastructure nodes | For production systems |
| - | **C4Dynamic** | Technical | Request flows (numbered) | For complex workflows |

**Key Insight:** "Context + Container diagrams are sufficient for most software development teams." Only create Component/Code diagrams when they genuinely add value.

## Quick Start Examples

### System Context (Level 1)
\`\`\`mermaid
C4Context
  title System Context - Workout Tracker

  Person(user, "User", "Tracks workouts and exercises")
  System(app, "Workout Tracker", "Vue PWA for tracking strength and CrossFit workouts")
  System_Ext(browser, "Web Browser", "Stores data in IndexedDB")

  Rel(user, app, "Uses")
  Rel(app, browser, "Persists data to", "IndexedDB")
\`\`\`

### Container Diagram (Level 2)
\`\`\`mermaid
C4Container
  title Container Diagram - Workout Tracker

  Person(user, "User", "Tracks workouts")

  Container_Boundary(app, "Workout Tracker PWA") {
    Container(spa, "SPA", "Vue 3, TypeScript", "Single-page application")
    Container(pinia, "State Management", "Pinia", "Manages application state")
    ContainerDb(indexeddb, "IndexedDB", "Dexie", "Local workout storage")
  }

  Rel(user, spa, "Uses")
  Rel(spa, pinia, "Reads/writes state")
  Rel(pinia, indexeddb, "Persists", "Dexie ORM")
\`\`\`

### Component Diagram (Level 3)
\`\`\`mermaid
C4Component
  title Component Diagram - Workout Feature

  Container(views, "Views", "Vue Router pages")

  Container_Boundary(workout, "Workout Feature") {
    Component(useWorkout, "useWorkout", "Composable", "Workout execution state")
    Component(useTimer, "useTimer", "Composable", "Timer state machine")
    Component(workoutRepo, "WorkoutRepository", "Dexie", "Workout persistence")
  }

  Rel(views, useWorkout, "Uses")
  Rel(useWorkout, useTimer, "Controls")
  Rel(useWorkout, workoutRepo, "Saves to")
\`\`\`

### Dynamic Diagram (Request Flow)
\`\`\`mermaid
C4Dynamic
  title Dynamic Diagram - User Sign In Flow

  ContainerDb(db, "Database", "PostgreSQL", "User credentials")
  Container(spa, "Single-Page App", "React", "Banking UI")

  Container_Boundary(api, "API Application") {
    Component(signIn, "Sign In Controller", "Express", "Auth endpoint")
    Component(security, "Security Service", "JWT", "Validates credentials")
  }

  Rel(spa, signIn, "1. Submit credentials", "JSON/HTTPS")
  Rel(signIn, security, "2. Validate")
  Rel(security, db, "3. Query user", "SQL")

  UpdateRelStyle(spa, signIn, \$textColor="blue", \$offsetY="-30")
\`\`\`

### Deployment Diagram
\`\`\`mermaid
C4Deployment
  title Deployment Diagram - Production

  Deployment_Node(browser, "Customer Browser", "Chrome/Firefox") {
    Container(spa, "SPA", "React", "Web application")
  }

  Deployment_Node(aws, "AWS Cloud", "us-east-1") {
    Deployment_Node(ecs, "ECS Cluster", "Fargate") {
      Container(api, "API Service", "Node.js", "REST API")
    }
    Deployment_Node(rds, "RDS", "db.r5.large") {
      ContainerDb(db, "Database", "PostgreSQL", "Application data")
    }
  }

  Rel(spa, api, "API calls", "HTTPS")
  Rel(api, db, "Reads/writes", "JDBC")
\`\`\`

## Element Syntax

### People and Systems
\`\`\`
Person(alias, "Label", "Description")
Person_Ext(alias, "Label", "Description")       # External person
System(alias, "Label", "Description")
System_Ext(alias, "Label", "Description")       # External system
SystemDb(alias, "Label", "Description")         # Database system
SystemQueue(alias, "Label", "Description")      # Queue system
\`\`\`

### Containers
\`\`\`
Container(alias, "Label", "Technology", "Description")
Container_Ext(alias, "Label", "Technology", "Description")
ContainerDb(alias, "Label", "Technology", "Description")
ContainerQueue(alias, "Label", "Technology", "Description")
\`\`\`

### Components
\`\`\`
Component(alias, "Label", "Technology", "Description")
Component_Ext(alias, "Label", "Technology", "Description")
ComponentDb(alias, "Label", "Technology", "Description")
\`\`\`

### Boundaries
\`\`\`
Enterprise_Boundary(alias, "Label") { ... }
System_Boundary(alias, "Label") { ... }
Container_Boundary(alias, "Label") { ... }
Boundary(alias, "Label", "type") { ... }
\`\`\`

### Relationships
\`\`\`
Rel(from, to, "Label")
Rel(from, to, "Label", "Technology")
BiRel(from, to, "Label")                        # Bidirectional
Rel_U(from, to, "Label")                        # Upward
Rel_D(from, to, "Label")                        # Downward
Rel_L(from, to, "Label")                        # Leftward
Rel_R(from, to, "Label")                        # Rightward
\`\`\`

### Deployment Nodes
\`\`\`
Deployment_Node(alias, "Label", "Type", "Description") { ... }
Node(alias, "Label", "Type", "Description") { ... }  # Shorthand
\`\`\`

## Styling and Layout

### Layout Configuration
\`\`\`
UpdateLayoutConfig(\$c4ShapeInRow="3", \$c4BoundaryInRow="1")
\`\`\`
- \`\$c4ShapeInRow\` - Number of shapes per row (default: 4)
- \`\$c4BoundaryInRow\` - Number of boundaries per row (default: 2)

### Element Styling
\`\`\`
UpdateElementStyle(alias, \$fontColor="red", \$bgColor="grey", \$borderColor="red")
\`\`\`

### Relationship Styling
\`\`\`
UpdateRelStyle(from, to, \$textColor="blue", \$lineColor="blue", \$offsetX="5", \$offsetY="-10")
\`\`\`
Use \`\$offsetX\` and \`\$offsetY\` to fix overlapping relationship labels.

## Best Practices

### Essential Rules

1. **Every element must have**: Name, Type, Technology (where applicable), and Description
2. **Use unidirectional arrows only** - Bidirectional arrows create ambiguity
3. **Label arrows with action verbs** - "Sends email using", "Reads from", not just "uses"
4. **Include technology labels** - "JSON/HTTPS", "JDBC", "gRPC"
5. **Stay under 20 elements per diagram** - Split complex systems into multiple diagrams

### Clarity Guidelines

1. **Start at Level 1** - Context diagrams help frame the system scope
2. **One diagram per file** - Keep diagrams focused on a single abstraction level
3. **Meaningful aliases** - Use descriptive aliases (e.g., \`orderService\` not \`s1\`)
4. **Concise descriptions** - Keep descriptions under 50 characters when possible
5. **Always include a title** - "System Context diagram for [System Name]"

### What to Avoid

See [references/common-mistakes.md](references/common-mistakes.md) for detailed anti-patterns:
- Confusing containers (deployable) vs components (non-deployable)
- Modeling shared libraries as containers
- Showing message brokers as single containers instead of individual topics
- Adding undefined abstraction levels like "subcomponents"
- Removing type labels to "simplify" diagrams

## Microservices Guidelines

### Single Team Ownership
Model each microservice as a **container** (or container group):
\`\`\`mermaid
C4Container
  title Microservices - Single Team

  System_Boundary(platform, "E-commerce Platform") {
    Container(orderApi, "Order Service", "Spring Boot", "Order processing")
    ContainerDb(orderDb, "Order DB", "PostgreSQL", "Order data")
    Container(inventoryApi, "Inventory Service", "Node.js", "Stock management")
    ContainerDb(inventoryDb, "Inventory DB", "MongoDB", "Stock data")
  }
\`\`\`

### Multi-Team Ownership
Promote microservices to **software systems** when owned by separate teams:
\`\`\`mermaid
C4Context
  title Microservices - Multi-Team

  Person(customer, "Customer", "Places orders")
  System(orderSystem, "Order System", "Team Alpha")
  System(inventorySystem, "Inventory System", "Team Beta")
  System(paymentSystem, "Payment System", "Team Gamma")

  Rel(customer, orderSystem, "Places orders")
  Rel(orderSystem, inventorySystem, "Checks stock")
  Rel(orderSystem, paymentSystem, "Processes payment")
\`\`\`

### Event-Driven Architecture
Show individual topics/queues as containers, NOT a single "Kafka" box:
\`\`\`mermaid
C4Container
  title Event-Driven Architecture

  Container(orderService, "Order Service", "Java", "Creates orders")
  Container(stockService, "Stock Service", "Java", "Manages inventory")
  ContainerQueue(orderTopic, "order.created", "Kafka", "Order events")
  ContainerQueue(stockTopic, "stock.reserved", "Kafka", "Stock events")

  Rel(orderService, orderTopic, "Publishes to")
  Rel(stockService, orderTopic, "Subscribes to")
  Rel(stockService, stockTopic, "Publishes to")
  Rel(orderService, stockTopic, "Subscribes to")
\`\`\`

## Output Location

Write architecture documentation to \`docs/architecture/\` with naming convention:
- \`c4-context.md\` - System context diagram
- \`c4-containers.md\` - Container diagram
- \`c4-components-{feature}.md\` - Component diagrams per feature
- \`c4-deployment.md\` - Deployment diagram
- \`c4-dynamic-{flow}.md\` - Dynamic diagrams for specific flows

## Audience-Appropriate Detail

| Audience | Recommended Diagrams |
|----------|---------------------|
| Executives | System Context only |
| Product Managers | Context + Container |
| Architects | Context + Container + key Components |
| Developers | All levels as needed |
| DevOps | Container + Deployment |

## References

- [references/c4-syntax.md](references/c4-syntax.md) - Complete Mermaid C4 syntax
- [references/common-mistakes.md](references/common-mistakes.md) - Anti-patterns to avoid
- [references/advanced-patterns.md](references/advanced-patterns.md) - Microservices, event-driven, deployment
`,
  "run-tests-and-fix": `---
name: run-tests-and-fix
description: Run Tests and Fix Failures
disable-model-invocation: true
---

# Run Tests and Fix Failures

## Overview

Execute the full test suite and systematically fix any failures, ensuring code quality and functionality.

## Steps

1. **Run test suite**
   - Execute all tests in the project
   - Capture output and identify failures
   - Check both unit and integration tests

2. **Analyze failures**
   - Categorize by type: flaky, broken, new failures
   - Prioritize fixes based on impact
   - Check if failures are related to recent changes

3. **Fix issues systematically**
   - Start with the most critical failures
   - Fix one issue at a time
   - Re-run tests after each fix

## Test recovery checklist

- [ ] Full test suite executed
- [ ] Failures categorized and tracked
- [ ] Root causes resolved
- [ ] Tests re-run with passing results
- [ ] Follow-up improvements noted
`,
  "security-audit-copy": `---
name: security-audit-copy
description: Security Audit
disable-model-invocation: true
---

# Security Audit

## Overview

Comprehensive security review to identify and fix vulnerabilities in the
codebase.

## Steps

1. **Dependency audit**
   - Check for known vulnerabilities
   - Update outdated packages
   - Review third-party dependencies
2. **Code security review**
   - Check for common vulnerabilities
   - Review authentication/authorization
   - Audit data handling practices
3. **Infrastructure security**
   - Review environment variables
   - Check access controls
   - Audit network security

## Security Checklist

- [ ] Dependencies updated and secure
- [ ] No hardcoded secrets
- [ ] Input validation implemented
- [ ] Authentication secure
- [ ] Authorization properly configured
`,
  "security-audit": `---
name: security-audit
description: Security Audit
disable-model-invocation: true
---

# Security Audit

## Overview

Perform a comprehensive security review of the codebase: identify vulnerabilities, then provide specific remediation steps with code examples for each issue. Cover dependencies, code patterns, data handling, and infrastructure.

## Steps

1. **Dependency audit**
   - Check for known vulnerabilities (e.g. \`npm audit\`)
   - Update outdated packages
   - Review third-party dependencies

2. **Authentication & authorization**
   - Verify proper authentication mechanisms
   - Check authorization controls and permission systems (e.g. Convex auth guards, membership checks)
   - Review session management and token handling
   - Ensure secure password policies and storage (if applicable)

3. **Input validation & sanitization**
   - Identify SQL injection and other injection vulnerabilities
   - Check for XSS and CSRF attack vectors
   - Validate all user inputs and API parameters
   - Review file upload and processing security

4. **Data protection**
   - Ensure sensitive data encryption at rest and in transit
   - Check for data exposure in logs and error messages
   - Review API responses for information leakage
   - Verify proper secrets management; no hardcoded secrets

5. **Infrastructure security**
   - Review environment variables and configuration security
   - Check HTTPS configuration and certificate validation
   - Analyze CORS policies and security headers
   - Audit network and access controls

## Security checklist

- [ ] Dependencies updated and free of known vulnerabilities
- [ ] No hardcoded secrets; proper secrets management
- [ ] Input validation and sanitization implemented
- [ ] Authentication mechanisms verified
- [ ] Authorization and permission systems checked
- [ ] Session management and token handling reviewed
- [ ] Sensitive data encrypted at rest and in transit
- [ ] No sensitive data in logs or error messages
- [ ] CORS and security headers reviewed
- [ ] Environment and configuration security reviewed
`,
  "sprint-planning": `---
name: writing-plans
description: Use when you have a spec or requirements for a multi-step task, before touching code
---

# Writing Plans

## Overview

Write comprehensive implementation plans assuming the engineer has zero context for our codebase and questionable taste. Document everything they need to know: which files to touch for each task, code, testing, docs they might need to check, how to test it. Give them the whole plan as bite-sized tasks. DRY. YAGNI. TDD. Frequent commits.

Assume they are a skilled developer, but know almost nothing about our toolset or problem domain. Assume they don't know good test design very well.

**Announce at start:** "I'm using the writing-plans skill to create the implementation plan."

**Context:** This should be run in a dedicated worktree (created by brainstorming skill).

**Save plans to:** \`docs/plans/YYYY-MM-DD-<feature-name>.md\`

## Bite-Sized Task Granularity

**Each step is one action (2-5 minutes):**
- "Write the failing test" - step
- "Run it to make sure it fails" - step
- "Implement the minimal code to make the test pass" - step
- "Run the tests and make sure they pass" - step
- "Commit" - step

## Plan Document Header

**Every plan MUST start with this header:**

\`\`\`markdown
# [Feature Name] Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** [One sentence describing what this builds]

**Architecture:** [2-3 sentences about approach]

**Tech Stack:** [Key technologies/libraries]

---
\`\`\`

## Task Structure

\`\`\`markdown
### Task N: [Component Name]

**Files:**
- Create: \`exact/path/to/file.py\`
- Modify: \`exact/path/to/existing.py:123-145\`
- Test: \`tests/exact/path/to/test.py\`

**Step 1: Write the failing test**

\`\`\`python
def test_specific_behavior():
    result = function(input)
    assert result == expected
\`\`\`

**Step 2: Run test to verify it fails**

Run: \`pytest tests/path/test.py::test_name -v\`
Expected: FAIL with "function not defined"

**Step 3: Write minimal implementation**

\`\`\`python
def function(input):
    return expected
\`\`\`

**Step 4: Run test to verify it passes**

Run: \`pytest tests/path/test.py::test_name -v\`
Expected: PASS

**Step 5: Commit**

\`\`\`bash
git add tests/path/test.py src/path/file.py
git commit -m "feat: add specific feature"
\`\`\`
\`\`\`

## Remember
- Exact file paths always
- Complete code in plan (not "add validation")
- Exact commands with expected output
- Reference relevant skills with @ syntax
- DRY, YAGNI, TDD, frequent commits

## Execution Handoff

After saving the plan, offer execution choice:

**"Plan complete and saved to \`docs/plans/<filename>.md\`. Two execution options:**

**1. Subagent-Driven (this session)** - I dispatch fresh subagent per task, review between tasks, fast iteration

**2. Parallel Session (separate)** - Open new session with executing-plans, batch execution with checkpoints

**Which approach?"**

**If Subagent-Driven chosen:**
- **REQUIRED SUB-SKILL:** Use superpowers:subagent-driven-development
- Stay in this session
- Fresh subagent per task + code review

**If Parallel Session chosen:**
- Guide them to open new session in worktree
- **REQUIRED SUB-SKILL:** New session uses superpowers:executing-plans
`,
  "test-automation": `---
name: webapp-testing
description: Toolkit for interacting with and testing local web applications using Playwright. Supports verifying frontend functionality, debugging UI behavior, capturing browser screenshots, and viewing browser logs.
license: Complete terms in LICENSE.txt
---

# Web Application Testing

To test local web applications, write native Python Playwright scripts.

**Helper Scripts Available**:
- \`scripts/with_server.py\` - Manages server lifecycle (supports multiple servers)

**Always run scripts with \`--help\` first** to see usage. DO NOT read the source until you try running the script first and find that a customized solution is abslutely necessary. These scripts can be very large and thus pollute your context window. They exist to be called directly as black-box scripts rather than ingested into your context window.

## Decision Tree: Choosing Your Approach

\`\`\`
User task ‚Üí Is it static HTML?
    ‚îú‚îÄ Yes ‚Üí Read HTML file directly to identify selectors
    ‚îÇ         ‚îú‚îÄ Success ‚Üí Write Playwright script using selectors
    ‚îÇ         ‚îî‚îÄ Fails/Incomplete ‚Üí Treat as dynamic (below)
    ‚îÇ
    ‚îî‚îÄ No (dynamic webapp) ‚Üí Is the server already running?
        ‚îú‚îÄ No ‚Üí Run: python scripts/with_server.py --help
        ‚îÇ        Then use the helper + write simplified Playwright script
        ‚îÇ
        ‚îî‚îÄ Yes ‚Üí Reconnaissance-then-action:
            1. Navigate and wait for networkidle
            2. Take screenshot or inspect DOM
            3. Identify selectors from rendered state
            4. Execute actions with discovered selectors
\`\`\`

## Example: Using with_server.py

To start a server, run \`--help\` first, then use the helper:

**Single server:**
\`\`\`bash
python scripts/with_server.py --server "npm run dev" --port 5173 -- python your_automation.py
\`\`\`

**Multiple servers (e.g., backend + frontend):**
\`\`\`bash
python scripts/with_server.py \\
  --server "cd backend && python server.py" --port 3000 \\
  --server "cd frontend && npm run dev" --port 5173 \\
  -- python your_automation.py
\`\`\`

To create an automation script, include only Playwright logic (servers are managed automatically):
\`\`\`python
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(headless=True) # Always launch chromium in headless mode
    page = browser.new_page()
    page.goto('http://localhost:5173') # Server already running and ready
    page.wait_for_load_state('networkidle') # CRITICAL: Wait for JS to execute
    # ... your automation logic
    browser.close()
\`\`\`

## Reconnaissance-Then-Action Pattern

1. **Inspect rendered DOM**:
   \`\`\`python
   page.screenshot(path='/tmp/inspect.png', full_page=True)
   content = page.content()
   page.locator('button').all()
   \`\`\`

2. **Identify selectors** from inspection results

3. **Execute actions** using discovered selectors

## Common Pitfall

‚ùå **Don't** inspect the DOM before waiting for \`networkidle\` on dynamic apps
‚úÖ **Do** wait for \`page.wait_for_load_state('networkidle')\` before inspection

## Best Practices

- **Use bundled scripts as black boxes** - To accomplish a task, consider whether one of the scripts available in \`scripts/\` can help. These scripts handle common, complex workflows reliably without cluttering the context window. Use \`--help\` to see usage, then invoke directly. 
- Use \`sync_playwright()\` for synchronous scripts
- Always close the browser when done
- Use descriptive selectors: \`text=\`, \`role=\`, CSS selectors, or IDs
- Add appropriate waits: \`page.wait_for_selector()\` or \`page.wait_for_timeout()\`

## Reference Files

- **examples/** - Examples showing common patterns:
  - \`element_discovery.py\` - Discovering buttons, links, and inputs on a page
  - \`static_html_automation.py\` - Using file:// URLs for local HTML
  - \`console_logging.py\` - Capturing console logs during automation`,
  "test-strategy": `---
name: test-driven-development
description: Use when implementing any feature or bugfix, before writing implementation code
---

# Test-Driven Development (TDD)

## Overview

Write the test first. Watch it fail. Write minimal code to pass.

**Core principle:** If you didn't watch the test fail, you don't know if it tests the right thing.

**Violating the letter of the rules is violating the spirit of the rules.**

## When to Use

**Always:**
- New features
- Bug fixes
- Refactoring
- Behavior changes

**Exceptions (ask your human partner):**
- Throwaway prototypes
- Generated code
- Configuration files

Thinking "skip TDD just this once"? Stop. That's rationalization.

## The Iron Law

\`\`\`
NO PRODUCTION CODE WITHOUT A FAILING TEST FIRST
\`\`\`

Write code before the test? Delete it. Start over.

**No exceptions:**
- Don't keep it as "reference"
- Don't "adapt" it while writing tests
- Don't look at it
- Delete means delete

Implement fresh from tests. Period.

## Red-Green-Refactor

\`\`\`dot
digraph tdd_cycle {
    rankdir=LR;
    red [label="RED\\nWrite failing test", shape=box, style=filled, fillcolor="#ffcccc"];
    verify_red [label="Verify fails\\ncorrectly", shape=diamond];
    green [label="GREEN\\nMinimal code", shape=box, style=filled, fillcolor="#ccffcc"];
    verify_green [label="Verify passes\\nAll green", shape=diamond];
    refactor [label="REFACTOR\\nClean up", shape=box, style=filled, fillcolor="#ccccff"];
    next [label="Next", shape=ellipse];

    red -> verify_red;
    verify_red -> green [label="yes"];
    verify_red -> red [label="wrong\\nfailure"];
    green -> verify_green;
    verify_green -> refactor [label="yes"];
    verify_green -> green [label="no"];
    refactor -> verify_green [label="stay\\ngreen"];
    verify_green -> next;
    next -> red;
}
\`\`\`

### RED - Write Failing Test

Write one minimal test showing what should happen.

<Good>
\`\`\`typescript
test('retries failed operations 3 times', async () => {
  let attempts = 0;
  const operation = () => {
    attempts++;
    if (attempts < 3) throw new Error('fail');
    return 'success';
  };

  const result = await retryOperation(operation);

  expect(result).toBe('success');
  expect(attempts).toBe(3);
});
\`\`\`
Clear name, tests real behavior, one thing
</Good>

<Bad>
\`\`\`typescript
test('retry works', async () => {
  const mock = jest.fn()
    .mockRejectedValueOnce(new Error())
    .mockRejectedValueOnce(new Error())
    .mockResolvedValueOnce('success');
  await retryOperation(mock);
  expect(mock).toHaveBeenCalledTimes(3);
});
\`\`\`
Vague name, tests mock not code
</Bad>

**Requirements:**
- One behavior
- Clear name
- Real code (no mocks unless unavoidable)

### Verify RED - Watch It Fail

**MANDATORY. Never skip.**

\`\`\`bash
npm test path/to/test.test.ts
\`\`\`

Confirm:
- Test fails (not errors)
- Failure message is expected
- Fails because feature missing (not typos)

**Test passes?** You're testing existing behavior. Fix test.

**Test errors?** Fix error, re-run until it fails correctly.

### GREEN - Minimal Code

Write simplest code to pass the test.

<Good>
\`\`\`typescript
async function retryOperation<T>(fn: () => Promise<T>): Promise<T> {
  for (let i = 0; i < 3; i++) {
    try {
      return await fn();
    } catch (e) {
      if (i === 2) throw e;
    }
  }
  throw new Error('unreachable');
}
\`\`\`
Just enough to pass
</Good>

<Bad>
\`\`\`typescript
async function retryOperation<T>(
  fn: () => Promise<T>,
  options?: {
    maxRetries?: number;
    backoff?: 'linear' | 'exponential';
    onRetry?: (attempt: number) => void;
  }
): Promise<T> {
  // YAGNI
}
\`\`\`
Over-engineered
</Bad>

Don't add features, refactor other code, or "improve" beyond the test.

### Verify GREEN - Watch It Pass

**MANDATORY.**

\`\`\`bash
npm test path/to/test.test.ts
\`\`\`

Confirm:
- Test passes
- Other tests still pass
- Output pristine (no errors, warnings)

**Test fails?** Fix code, not test.

**Other tests fail?** Fix now.

### REFACTOR - Clean Up

After green only:
- Remove duplication
- Improve names
- Extract helpers

Keep tests green. Don't add behavior.

### Repeat

Next failing test for next feature.

## Good Tests

| Quality | Good | Bad |
|---------|------|-----|
| **Minimal** | One thing. "and" in name? Split it. | \`test('validates email and domain and whitespace')\` |
| **Clear** | Name describes behavior | \`test('test1')\` |
| **Shows intent** | Demonstrates desired API | Obscures what code should do |

## Why Order Matters

**"I'll write tests after to verify it works"**

Tests written after code pass immediately. Passing immediately proves nothing:
- Might test wrong thing
- Might test implementation, not behavior
- Might miss edge cases you forgot
- You never saw it catch the bug

Test-first forces you to see the test fail, proving it actually tests something.

**"I already manually tested all the edge cases"**

Manual testing is ad-hoc. You think you tested everything but:
- No record of what you tested
- Can't re-run when code changes
- Easy to forget cases under pressure
- "It worked when I tried it" ‚â† comprehensive

Automated tests are systematic. They run the same way every time.

**"Deleting X hours of work is wasteful"**

Sunk cost fallacy. The time is already gone. Your choice now:
- Delete and rewrite with TDD (X more hours, high confidence)
- Keep it and add tests after (30 min, low confidence, likely bugs)

The "waste" is keeping code you can't trust. Working code without real tests is technical debt.

**"TDD is dogmatic, being pragmatic means adapting"**

TDD IS pragmatic:
- Finds bugs before commit (faster than debugging after)
- Prevents regressions (tests catch breaks immediately)
- Documents behavior (tests show how to use code)
- Enables refactoring (change freely, tests catch breaks)

"Pragmatic" shortcuts = debugging in production = slower.

**"Tests after achieve the same goals - it's spirit not ritual"**

No. Tests-after answer "What does this do?" Tests-first answer "What should this do?"

Tests-after are biased by your implementation. You test what you built, not what's required. You verify remembered edge cases, not discovered ones.

Tests-first force edge case discovery before implementing. Tests-after verify you remembered everything (you didn't).

30 minutes of tests after ‚â† TDD. You get coverage, lose proof tests work.

## Common Rationalizations

| Excuse | Reality |
|--------|---------|
| "Too simple to test" | Simple code breaks. Test takes 30 seconds. |
| "I'll test after" | Tests passing immediately prove nothing. |
| "Tests after achieve same goals" | Tests-after = "what does this do?" Tests-first = "what should this do?" |
| "Already manually tested" | Ad-hoc ‚â† systematic. No record, can't re-run. |
| "Deleting X hours is wasteful" | Sunk cost fallacy. Keeping unverified code is technical debt. |
| "Keep as reference, write tests first" | You'll adapt it. That's testing after. Delete means delete. |
| "Need to explore first" | Fine. Throw away exploration, start with TDD. |
| "Test hard = design unclear" | Listen to test. Hard to test = hard to use. |
| "TDD will slow me down" | TDD faster than debugging. Pragmatic = test-first. |
| "Manual test faster" | Manual doesn't prove edge cases. You'll re-test every change. |
| "Existing code has no tests" | You're improving it. Add tests for existing code. |

## Red Flags - STOP and Start Over

- Code before test
- Test after implementation
- Test passes immediately
- Can't explain why test failed
- Tests added "later"
- Rationalizing "just this once"
- "I already manually tested it"
- "Tests after achieve the same purpose"
- "It's about spirit not ritual"
- "Keep as reference" or "adapt existing code"
- "Already spent X hours, deleting is wasteful"
- "TDD is dogmatic, I'm being pragmatic"
- "This is different because..."

**All of these mean: Delete code. Start over with TDD.**

## Example: Bug Fix

**Bug:** Empty email accepted

**RED**
\`\`\`typescript
test('rejects empty email', async () => {
  const result = await submitForm({ email: '' });
  expect(result.error).toBe('Email required');
});
\`\`\`

**Verify RED**
\`\`\`bash
\$ npm test
FAIL: expected 'Email required', got undefined
\`\`\`

**GREEN**
\`\`\`typescript
function submitForm(data: FormData) {
  if (!data.email?.trim()) {
    return { error: 'Email required' };
  }
  // ...
}
\`\`\`

**Verify GREEN**
\`\`\`bash
\$ npm test
PASS
\`\`\`

**REFACTOR**
Extract validation for multiple fields if needed.

## Verification Checklist

Before marking work complete:

- [ ] Every new function/method has a test
- [ ] Watched each test fail before implementing
- [ ] Each test failed for expected reason (feature missing, not typo)
- [ ] Wrote minimal code to pass each test
- [ ] All tests pass
- [ ] Output pristine (no errors, warnings)
- [ ] Tests use real code (mocks only if unavoidable)
- [ ] Edge cases and errors covered

Can't check all boxes? You skipped TDD. Start over.

## When Stuck

| Problem | Solution |
|---------|----------|
| Don't know how to test | Write wished-for API. Write assertion first. Ask your human partner. |
| Test too complicated | Design too complicated. Simplify interface. |
| Must mock everything | Code too coupled. Use dependency injection. |
| Test setup huge | Extract helpers. Still complex? Simplify design. |

## Debugging Integration

Bug found? Write failing test reproducing it. Follow TDD cycle. Test proves fix and prevents regression.

Never fix bugs without a test.

## Testing Anti-Patterns

When adding mocks or test utilities, read @testing-anti-patterns.md to avoid common pitfalls:
- Testing mock behavior instead of real behavior
- Adding test-only methods to production classes
- Mocking without understanding dependencies

## Final Rule

\`\`\`
Production code ‚Üí test exists and failed first
Otherwise ‚Üí not TDD
\`\`\`

No exceptions without your human partner's permission.
`,
};
